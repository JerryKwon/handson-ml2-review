{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. 강화 학습\n",
    "\n",
    "강화 학습(Reinforcement Learning)은 요즘 머신러닝에서 가장 흥미진진한 분야이자 가장 오래된 분야이다. 강화 학습이 현대에 다시 재조명을 받은 시기는 2013년에 딥마인드에서 시도한 <a href=\"https://arxiv.org/pdf/1312.5602.pdf?source=post_page---------------------------\">아타리게임들에 대해 수행된 연구</a>때문입니다. 이 연구에서 기계는 화면 픽셀에 대한 데이터만 입력으로 받고 <a href=\"https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf\">게임 규칙에 대한 어떤 사전 정보없이 대부분 사람을 능가하는 성과</a>를 냈다.\n",
    "\n",
    "딥마인드가 이러한 성과를 낼수 있던 이유는? 강화 학습 분야에 강력한 딥러닝을 적용했기 때문이다.\n",
    "\n",
    "18장에서는,\n",
    "* 강화 학습의 정의 및 활용 분야\n",
    "* 정책 그라디언트, 심층 Q-네트워크\n",
    "\n",
    "## 18.1. 보상을 최적화하기 위한 학습\n",
    "\n",
    "### 강화 학습의 구성요소\n",
    "\n",
    "* 에이전트: 인공지능 플레이어\n",
    "* 환경: 에이전트가 솔루션을 찾기 위한 무대\n",
    "* 행동: 에이전트가 환경 안에서 시행하는 상호작용\n",
    "* 보상: 에이전트의 행동에 따른 점수 혹은 결과\n",
    "\n",
    "위의 4가지 요소를 가지고 강화학습은 **'에이전트는 관측을 하고 주어진 환경에서 행동을 하고, 이에 대한 결과로 보상을 받는다'**라는 문장으로 요약할 수 있다. 에이전트는 환경 아래에서 시행착오를 겪으며 보상을 최대로 하는 방향으로 학습한다. 강화학습은 자율주행 자동차, 추천 시스템, 웹페이지에 광고 배치, 이미지 분류 시스템의 제어 등에 사용될 수 있다.\n",
    "\n",
    "## 18.2. 정책 탐색\n",
    "\n",
    "에이전트가 행동을 결정하기 위해 사용하는 알고리즘을 정책(policy)라고 한다. 아래의 그림과 같이 Agent가 위치한 상태를 입력으로 받고 행동을 출력하는 신경망이 정책이 될 수 있다.\n",
    "\n",
    "![RL_figure](../../img/RL_figure.jpg)\n",
    "\n",
    "### 강화 학습의 예시; 청소기\n",
    "\n",
    "* Agent: 30분 동안 수집한 먼지의 양을 보상으로 받는 로봇 진공청소기\n",
    "* 정책: 매 초마다 p의 확률로 전진 or (1-p)의 확률로 왼쪽 또는 오른쪽으로 랜덤하게 회전; 회전의 각도는 -r과 +r 사이의 랜덤한 각도\n",
    "\n",
    "**어떻게 훈련할 수 있을까?(정책탐색; Policy Search)**\n",
    "1. 무작위 방식: 정책 파라미터들에 대해 무작위로 시행을 수행하고 성능이 좋은 조합을 선택\n",
    "2. 유전 알고리즘(Genetic Algorithm): 1세대 정책 100개를 랜덤하여 생성하고, 하위 80개의 정책을 drop. 남은 20개를 활용하여 자식 정책 4개를 생성한다. 자식 정책 4개는 복사된 부모의 정책과 약간의 무작위 성을 설정한 것.\n",
    "3. 정책 그라디언트(Policy Gradient): 정책 파라미터에 대한 보상의 그라디언트를 평가하여 높은 보상의 방향을 따르는 그라디언트로 파라미터를 수정하는 최적화 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3. OpenAI 짐\n",
    "\n",
    "강화 학습 에이전트 훈련을 위한 최소한의 시뮬레이션 환경을 제공하는 패키지\n",
    "\n",
    "간단하게 확인해 볼 환경은 CartPole이라는 아타리의 게임 중 기울어지는 막대를 세우는 게임이다. \n",
    "\n",
    "CartPole 환경에서 return되는 관측값은 아래와 같이 구성된다.\n",
    "\n",
    "[수평 위치(0.0=중앙), 카트의 속도(양수=우측; 음수=좌측), 막대의 각도(0.0=수직), 막대의 각속도(양수=시계방향; 음수=반시계방향)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01207233, -0.0091725 ,  0.02314526,  0.00943016])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01225578,  0.18561   ,  0.02333387, -0.27586124])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:42.61; std:9.1301; min:25.0; max:72.0\n"
     ]
    }
   ],
   "source": [
    "rewards_mean = np.mean(totals)\n",
    "rewards_std = np.std(totals)\n",
    "rewards_min = np.min(totals)\n",
    "rewards_max = np.max(totals)\n",
    "\n",
    "print(\"mean:{:.2f}; std:{:.4f}; min:{}; max:{}\".format(rewards_mean,rewards_std,rewards_min,rewards_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 신경망 정책\n",
    "\n",
    "신경망 정책에서는 관측값을 통해 특정 결과에 대한 확률을 추정한다. 그리고 추정환 확률을 기반으로 랜덤으로 행동하도록 선택한다. 그렇다면 여기서 왜 '랜덤'하게 행동하도록 선택할까? 그 이유는 에이전트가 새로운 행동을 탐험하고 잘 할 수 있는 행동을 활용하는 행동을 유도하기 위함이다. \n",
    "\n",
    "일반적으로 각 관측은 환경에 대한 완전한 상태를 갖고 있기 때문에 과거 관측값에 대한 고려가 필요없다(e.g) CartPole). 그러나 관측에 잡음이 있는 경우에는 가능성있는 현재 상태의 추정을 위해 지난 관측 몇 개를 사용하는 것이 좋다.\n",
    "\n",
    "## 18.5. 행동 평가: 신용 할당 문제\n",
    "\n",
    "강화 학습에서는 일반적인 지도학습과는 달리 학습에 대한 평가 시, 실제 값 또는 Label이 주어지지 않는다. 다시 말해, 학습을 평가하는데 있어 사용되는 지표는 행동으로 주어지는 보상(reward)밖에 없다는 것이다. 그렇다면 에이전트가 수행한 각 행동에 대해 어떤 것이 좋고 나쁨을 구별할 수 있을까? 이는 신용 할당 문제(credit assignment problem)이라고 불린다.\n",
    "\n",
    "위의 문제를 해결하기 위해 행동이 일어난 후 각 단계마다 할인 계수(discount factor; $\\gamma$)를 적용한 보상을 모두 합하여 행동을 평가하는 것입니다. 이렇게 보상이 모두 합쳐진 값을 대가(return)이라고 부릅니다.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top:20px\">$0\\le\\gamma(할인 계수; discount factor)\\ge1$</div>\n",
    "<div align=\"center\" style=\"margin-top:10px\"><b>할인 계수 범위</b></div>\n",
    "\n",
    "할인 계수는 0과 1사이의 값으로 구성되며, 값이 클 수록 미래 시점에 주어지는 보상에 대해 우선순위롤 높게주는 것이고, 값이 낮을 수록 현재 시점에 주어지는 보상에 우선순위를 높게 주는 것이다. 일반적으로 $0.9~0.99$사이의 값을 준다.\n",
    "\n",
    "e.g) 0.95: 13step 이후의 보상 50% 할인; 0.99: 69step 이후의 보상 50% 할인\n",
    "\n",
    "위의 원리로 수행하게 된다면, 좋은 행동 후에 나쁜 행동이 이어져 낮은 대가를 받을 수 있다. 하지만, 평균적으로 다른 가능한 행동과 비교하여 각 행동이 얼마나 좋은지 혹은 나쁜지를 추정해야 한다. 이를 행동이익이라고 부르며, 많은 에피소드를 실행하여 모든 행동의 대가를 정규화해야 한다.\n",
    "\n",
    "## 18.6. 정책 그라디언트\n",
    "\n",
    "### REINFORCEMENT 알고리즘\n",
    "\n",
    "참고\n",
    "\n",
    "https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0\n",
    "\n",
    "https://github.com/g6ling/Reinforcement-Learning-Pytorch-Cartpole/tree/master/PG/1-REINFORCE\n",
    "\n",
    "https://wonseokjung.github.io/page5/\n",
    "\n",
    "1. 먼저 신경망 정책이 여러 번에 걸쳐 게임을 플레이하고 매 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그라디언트를 계산합니다. 하지만 그라디언트를 적용하지는 않는다.\n",
    "2. 에피소드를 몇 번 실행한 다음, 각 행동의 이익을 계산한다.\n",
    "3. 한 행동의 이익이 양수이면, 이 행동이 좋은 것임을 의미하므로 미래에 선택될 가능성이 높도록 앞서 계산한 그라디언트를 적용합니다. 그러나 행동이익이 음수이면 이 행동이 나쁜 것임을 의미하므로 미래에 이 행동이 덜 선택되도록 반대의 그라디언트를 적용합니다. 이는 각 그라디언트 벡터와 그에 상응하는 행동의 이익을 곱하면 됩니다.\n",
    "4. 마지막으로 모든 결과 그라디언트 벡터를 평균 내어 경사 하강법 스텝을 수행합니다.\n",
    "\n",
    "![sudo_REINFORCMENT](../../img/sudo_REINFORCEMENT.png)\n",
    "\n",
    "**아래의 코드는 특정 크기의 episode 만큼 batch로 하여 loss를 update하지 않음. K batch size는 1으로 보면 됨.**\n",
    "\n",
    "### 주요 파라미터\n",
    "* step size\n",
    "* distcount rate\n",
    "* batch size\n",
    "* max epsiodes\n",
    "\n",
    "### 정책 손실 $L(\\theta)$\n",
    " 신경망을 통해 도출되는 값은 확률분포를 따른다. 그렇기 때문에 $\\pi(a | s,\\theta)$는 신경망에서 각 상태에 대해 확률의 평균값을 얻기 위함이다. 그리고 확률의 평균 값을 할인 계수로 곱하여 신경망의 기대값을 계산한다.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "goal_score = 200\n",
    "log_interval = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "\n",
    "    def sample(self):\n",
    "        memory = self.memory\n",
    "        return Transition(*zip(*memory)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from config import gamma\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(QNet, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc_2 = nn.Linear(128, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc_1(input))\n",
    "        policy = F.softmax(self.fc_2(x))\n",
    "        return policy\n",
    "\n",
    "    # 매 종료된 에피소드마다 각 행동을 평가하기 위한 함수\n",
    "    @classmethod\n",
    "    def train_model(cls, net, transitions, optimizer):\n",
    "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        \n",
    "        running_return = 0\n",
    "        \n",
    "#         tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "#          1.,  1.,  1.,  1.,  1., -1.])\n",
    "#         print(rewards)\n",
    "        \n",
    "        # 3: 각 state 별 미래가치를 현재가치로 할인한 보상의 값\n",
    "        # 각 state에서 추정한 action으로 얻은 reward를 역순으로하여 미래가치로 할인을 수행\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
    "            returns[t] = running_return\n",
    "            \n",
    "#         print(returns)\n",
    "#         tensor([ 1.6557e+01,  1.5714e+01,  1.4863e+01,  1.4003e+01,  1.3134e+01,\n",
    "#          1.2257e+01,  1.1370e+01,  1.0475e+01,  9.5708e+00,  8.6574e+00,\n",
    "#          7.7348e+00,  6.8028e+00,  5.8614e+00,  4.9105e+00,  3.9500e+00,\n",
    "#          2.9798e+00,  1.9998e+00,  1.0099e+00,  1.0000e-02, -1.0000e+00])\n",
    "\n",
    "        # 정책 그라디언트를 학습하기 위한 신경망의 input으로 매 agent의 step별 state를 투입\n",
    "        policies = net(states)\n",
    "        policies = policies.view(-1, net.num_outputs)\n",
    "        \n",
    "#      https://subinium.github.io/pytorch-Tensor-Variable/\n",
    "#      detach reason: pytorch에서 텐서를 복제하기 위한 방법. 복사하면서 이전 텐서의 gradient에는 영향을 미치지 않기 위해서 사용함.\n",
    "        log_policies = (torch.log(policies) * actions.detach()).sum(dim=1)\n",
    "        \n",
    "#     정책들 중에서도 선택된 action에 대한 정책만을 선정하여 loss 값에 사용\n",
    "        loss = (-log_policies * returns).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # 행동을 return하는 함수\n",
    "    def get_action(self, input):\n",
    "        policy = self.forward(input)\n",
    "        # 왼쪽(0)과 오른쪽(1) 가속을 할 확률을 return 받음\n",
    "        # 0: 왼쪽인 확률을 기준으로 랜덤으로 다음 action을 선정 => 이는 exploration 과 exploitng 사이의 균형을 맞추기 위함\n",
    "        policy = policy[0].data.numpy()\n",
    "\n",
    "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4\n",
      "action size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode | steps: 23 |score: 0.22\n",
      "10 episode | steps: 22 |score: 2.62\n",
      "20 episode | steps: 17 |score: 5.46\n",
      "30 episode | steps: 32 |score: 7.21\n",
      "40 episode | steps: 18 |score: 8.84\n",
      "50 episode | steps: 23 |score: 10.39\n",
      "60 episode | steps: 30 |score: 11.69\n",
      "70 episode | steps: 27 |score: 13.52\n",
      "80 episode | steps: 22 |score: 14.73\n",
      "90 episode | steps: 34 |score: 15.61\n",
      "100 episode | steps: 68 |score: 16.91\n",
      "110 episode | steps: 22 |score: 18.44\n",
      "120 episode | steps: 28 |score: 19.87\n",
      "130 episode | steps: 50 |score: 21.22\n",
      "140 episode | steps: 23 |score: 22.81\n",
      "150 episode | steps: 32 |score: 23.90\n",
      "160 episode | steps: 45 |score: 25.14\n",
      "170 episode | steps: 38 |score: 25.47\n",
      "180 episode | steps: 36 |score: 26.06\n",
      "190 episode | steps: 33 |score: 27.31\n",
      "200 episode | steps: 79 |score: 28.70\n",
      "210 episode | steps: 33 |score: 29.88\n",
      "220 episode | steps: 24 |score: 30.23\n",
      "230 episode | steps: 30 |score: 30.90\n",
      "240 episode | steps: 74 |score: 31.73\n",
      "250 episode | steps: 31 |score: 31.99\n",
      "260 episode | steps: 48 |score: 32.46\n",
      "270 episode | steps: 32 |score: 33.77\n",
      "280 episode | steps: 25 |score: 34.49\n",
      "290 episode | steps: 53 |score: 36.12\n",
      "300 episode | steps: 56 |score: 36.61\n",
      "310 episode | steps: 30 |score: 36.32\n",
      "320 episode | steps: 33 |score: 36.99\n",
      "330 episode | steps: 35 |score: 37.08\n",
      "340 episode | steps: 53 |score: 37.19\n",
      "350 episode | steps: 19 |score: 38.27\n",
      "360 episode | steps: 26 |score: 38.83\n",
      "370 episode | steps: 89 |score: 39.24\n",
      "380 episode | steps: 58 |score: 38.99\n",
      "390 episode | steps: 76 |score: 39.22\n",
      "400 episode | steps: 35 |score: 41.15\n",
      "410 episode | steps: 64 |score: 40.66\n",
      "420 episode | steps: 52 |score: 40.25\n",
      "430 episode | steps: 48 |score: 41.42\n",
      "440 episode | steps: 20 |score: 41.18\n",
      "450 episode | steps: 30 |score: 41.52\n",
      "460 episode | steps: 42 |score: 41.93\n",
      "470 episode | steps: 46 |score: 42.71\n",
      "480 episode | steps: 58 |score: 42.74\n",
      "490 episode | steps: 37 |score: 42.31\n",
      "500 episode | steps: 48 |score: 42.51\n",
      "510 episode | steps: 37 |score: 41.83\n",
      "520 episode | steps: 94 |score: 41.96\n",
      "530 episode | steps: 74 |score: 42.26\n",
      "540 episode | steps: 107 |score: 43.59\n",
      "550 episode | steps: 48 |score: 43.42\n",
      "560 episode | steps: 20 |score: 43.86\n",
      "570 episode | steps: 57 |score: 44.31\n",
      "580 episode | steps: 28 |score: 44.82\n",
      "590 episode | steps: 40 |score: 46.25\n",
      "600 episode | steps: 27 |score: 45.48\n",
      "610 episode | steps: 34 |score: 45.59\n",
      "620 episode | steps: 49 |score: 46.88\n",
      "630 episode | steps: 74 |score: 48.00\n",
      "640 episode | steps: 36 |score: 48.21\n",
      "650 episode | steps: 50 |score: 47.80\n",
      "660 episode | steps: 55 |score: 48.39\n",
      "670 episode | steps: 54 |score: 48.71\n",
      "680 episode | steps: 49 |score: 48.67\n",
      "690 episode | steps: 54 |score: 49.92\n",
      "700 episode | steps: 38 |score: 50.00\n",
      "710 episode | steps: 75 |score: 51.01\n",
      "720 episode | steps: 34 |score: 51.21\n",
      "730 episode | steps: 42 |score: 50.65\n",
      "740 episode | steps: 63 |score: 52.21\n",
      "750 episode | steps: 27 |score: 51.99\n",
      "760 episode | steps: 62 |score: 53.61\n",
      "770 episode | steps: 43 |score: 53.66\n",
      "780 episode | steps: 57 |score: 53.77\n",
      "790 episode | steps: 62 |score: 53.36\n",
      "800 episode | steps: 47 |score: 53.19\n",
      "810 episode | steps: 37 |score: 53.10\n",
      "820 episode | steps: 30 |score: 52.36\n",
      "830 episode | steps: 71 |score: 53.09\n",
      "840 episode | steps: 44 |score: 52.79\n",
      "850 episode | steps: 63 |score: 53.21\n",
      "860 episode | steps: 67 |score: 53.91\n",
      "870 episode | steps: 95 |score: 54.14\n",
      "880 episode | steps: 68 |score: 54.57\n",
      "890 episode | steps: 64 |score: 54.35\n",
      "900 episode | steps: 58 |score: 54.16\n",
      "910 episode | steps: 53 |score: 53.95\n",
      "920 episode | steps: 32 |score: 53.91\n",
      "930 episode | steps: 47 |score: 52.81\n",
      "940 episode | steps: 44 |score: 53.30\n",
      "950 episode | steps: 48 |score: 53.12\n",
      "960 episode | steps: 63 |score: 52.51\n",
      "970 episode | steps: 47 |score: 52.25\n",
      "980 episode | steps: 105 |score: 51.66\n",
      "990 episode | steps: 52 |score: 51.65\n",
      "1000 episode | steps: 50 |score: 51.21\n",
      "1010 episode | steps: 33 |score: 50.59\n",
      "1020 episode | steps: 29 |score: 50.96\n",
      "1030 episode | steps: 37 |score: 50.19\n",
      "1040 episode | steps: 70 |score: 49.69\n",
      "1050 episode | steps: 59 |score: 49.52\n",
      "1060 episode | steps: 79 |score: 50.04\n",
      "1070 episode | steps: 35 |score: 49.96\n",
      "1080 episode | steps: 37 |score: 50.15\n",
      "1090 episode | steps: 35 |score: 50.59\n",
      "1100 episode | steps: 94 |score: 51.20\n",
      "1110 episode | steps: 35 |score: 50.91\n",
      "1120 episode | steps: 44 |score: 51.45\n",
      "1130 episode | steps: 33 |score: 52.25\n",
      "1140 episode | steps: 27 |score: 53.47\n",
      "1150 episode | steps: 20 |score: 52.13\n",
      "1160 episode | steps: 40 |score: 51.87\n",
      "1170 episode | steps: 62 |score: 51.30\n",
      "1180 episode | steps: 76 |score: 52.34\n",
      "1190 episode | steps: 34 |score: 52.58\n",
      "1200 episode | steps: 61 |score: 52.21\n",
      "1210 episode | steps: 36 |score: 51.89\n",
      "1220 episode | steps: 22 |score: 52.12\n",
      "1230 episode | steps: 24 |score: 51.79\n",
      "1240 episode | steps: 28 |score: 52.24\n",
      "1250 episode | steps: 53 |score: 53.20\n",
      "1260 episode | steps: 36 |score: 52.39\n",
      "1270 episode | steps: 28 |score: 52.99\n",
      "1280 episode | steps: 28 |score: 53.10\n",
      "1290 episode | steps: 89 |score: 53.60\n",
      "1300 episode | steps: 96 |score: 54.99\n",
      "1310 episode | steps: 106 |score: 56.55\n",
      "1320 episode | steps: 72 |score: 56.07\n",
      "1330 episode | steps: 35 |score: 56.26\n",
      "1340 episode | steps: 51 |score: 55.20\n",
      "1350 episode | steps: 74 |score: 56.98\n",
      "1360 episode | steps: 98 |score: 57.56\n",
      "1370 episode | steps: 26 |score: 56.48\n",
      "1380 episode | steps: 31 |score: 57.98\n",
      "1390 episode | steps: 36 |score: 58.81\n",
      "1400 episode | steps: 36 |score: 58.87\n",
      "1410 episode | steps: 86 |score: 59.55\n",
      "1420 episode | steps: 114 |score: 60.87\n",
      "1430 episode | steps: 64 |score: 60.00\n",
      "1440 episode | steps: 78 |score: 61.13\n",
      "1450 episode | steps: 51 |score: 61.22\n",
      "1460 episode | steps: 50 |score: 60.63\n",
      "1470 episode | steps: 102 |score: 59.27\n",
      "1480 episode | steps: 33 |score: 59.15\n",
      "1490 episode | steps: 42 |score: 59.32\n",
      "1500 episode | steps: 58 |score: 58.41\n",
      "1510 episode | steps: 45 |score: 58.32\n",
      "1520 episode | steps: 107 |score: 59.18\n",
      "1530 episode | steps: 94 |score: 61.11\n",
      "1540 episode | steps: 38 |score: 62.45\n",
      "1550 episode | steps: 93 |score: 62.55\n",
      "1560 episode | steps: 46 |score: 63.10\n",
      "1570 episode | steps: 77 |score: 62.62\n",
      "1580 episode | steps: 84 |score: 62.38\n",
      "1590 episode | steps: 90 |score: 62.32\n",
      "1600 episode | steps: 40 |score: 62.67\n",
      "1610 episode | steps: 33 |score: 62.92\n",
      "1620 episode | steps: 36 |score: 62.67\n",
      "1630 episode | steps: 78 |score: 62.18\n",
      "1640 episode | steps: 43 |score: 61.38\n",
      "1650 episode | steps: 46 |score: 61.40\n",
      "1660 episode | steps: 90 |score: 62.23\n",
      "1670 episode | steps: 24 |score: 61.80\n",
      "1680 episode | steps: 90 |score: 62.97\n",
      "1690 episode | steps: 19 |score: 63.35\n",
      "1700 episode | steps: 87 |score: 64.62\n",
      "1710 episode | steps: 44 |score: 65.79\n",
      "1720 episode | steps: 27 |score: 67.06\n",
      "1730 episode | steps: 103 |score: 67.70\n",
      "1740 episode | steps: 103 |score: 68.30\n",
      "1750 episode | steps: 109 |score: 68.91\n",
      "1760 episode | steps: 29 |score: 69.18\n",
      "1770 episode | steps: 104 |score: 68.87\n",
      "1780 episode | steps: 87 |score: 69.48\n",
      "1790 episode | steps: 113 |score: 71.10\n",
      "1800 episode | steps: 106 |score: 71.08\n",
      "1810 episode | steps: 31 |score: 71.51\n",
      "1820 episode | steps: 106 |score: 72.35\n",
      "1830 episode | steps: 104 |score: 71.59\n",
      "1840 episode | steps: 44 |score: 72.77\n",
      "1850 episode | steps: 17 |score: 72.83\n",
      "1860 episode | steps: 41 |score: 72.10\n",
      "1870 episode | steps: 25 |score: 72.06\n",
      "1880 episode | steps: 98 |score: 70.46\n",
      "1890 episode | steps: 117 |score: 72.34\n",
      "1900 episode | steps: 102 |score: 74.29\n",
      "1910 episode | steps: 105 |score: 76.38\n",
      "1920 episode | steps: 83 |score: 77.10\n",
      "1930 episode | steps: 84 |score: 76.90\n",
      "1940 episode | steps: 63 |score: 75.65\n",
      "1950 episode | steps: 110 |score: 74.48\n",
      "1960 episode | steps: 110 |score: 75.06\n",
      "1970 episode | steps: 91 |score: 74.72\n",
      "1980 episode | steps: 94 |score: 73.91\n",
      "1990 episode | steps: 107 |score: 73.06\n",
      "2000 episode | steps: 32 |score: 71.22\n",
      "2010 episode | steps: 37 |score: 69.05\n",
      "2020 episode | steps: 31 |score: 67.78\n",
      "2030 episode | steps: 98 |score: 69.23\n",
      "2040 episode | steps: 92 |score: 69.71\n",
      "2050 episode | steps: 49 |score: 68.27\n",
      "2060 episode | steps: 107 |score: 69.73\n",
      "2070 episode | steps: 102 |score: 71.37\n",
      "2080 episode | steps: 100 |score: 71.86\n",
      "2090 episode | steps: 99 |score: 73.58\n",
      "2100 episode | steps: 43 |score: 75.02\n",
      "2110 episode | steps: 93 |score: 76.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120 episode | steps: 109 |score: 77.61\n",
      "2130 episode | steps: 107 |score: 80.41\n",
      "2140 episode | steps: 106 |score: 82.63\n",
      "2150 episode | steps: 105 |score: 83.99\n",
      "2160 episode | steps: 38 |score: 83.85\n",
      "2170 episode | steps: 40 |score: 83.54\n",
      "2180 episode | steps: 105 |score: 83.38\n",
      "2190 episode | steps: 46 |score: 82.32\n",
      "2200 episode | steps: 112 |score: 80.77\n",
      "2210 episode | steps: 101 |score: 80.87\n",
      "2220 episode | steps: 103 |score: 82.27\n",
      "2230 episode | steps: 17 |score: 82.40\n",
      "2240 episode | steps: 96 |score: 81.17\n",
      "2250 episode | steps: 37 |score: 82.23\n",
      "2260 episode | steps: 88 |score: 82.62\n",
      "2270 episode | steps: 101 |score: 82.14\n",
      "2280 episode | steps: 41 |score: 79.77\n",
      "2290 episode | steps: 79 |score: 80.21\n",
      "2300 episode | steps: 94 |score: 81.52\n",
      "2310 episode | steps: 96 |score: 80.69\n",
      "2320 episode | steps: 95 |score: 82.46\n",
      "2330 episode | steps: 100 |score: 83.64\n",
      "2340 episode | steps: 93 |score: 85.32\n",
      "2350 episode | steps: 100 |score: 86.29\n",
      "2360 episode | steps: 38 |score: 87.74\n",
      "2370 episode | steps: 33 |score: 87.91\n",
      "2380 episode | steps: 101 |score: 87.80\n",
      "2390 episode | steps: 105 |score: 88.61\n",
      "2400 episode | steps: 100 |score: 89.87\n",
      "2410 episode | steps: 34 |score: 89.70\n",
      "2420 episode | steps: 112 |score: 91.01\n",
      "2430 episode | steps: 109 |score: 90.76\n",
      "2440 episode | steps: 102 |score: 90.67\n",
      "2450 episode | steps: 105 |score: 91.56\n",
      "2460 episode | steps: 22 |score: 91.29\n",
      "2470 episode | steps: 111 |score: 90.57\n",
      "2480 episode | steps: 23 |score: 88.70\n",
      "2490 episode | steps: 97 |score: 87.95\n",
      "2500 episode | steps: 112 |score: 87.32\n",
      "2510 episode | steps: 41 |score: 86.26\n",
      "2520 episode | steps: 46 |score: 84.60\n",
      "2530 episode | steps: 104 |score: 84.71\n",
      "2540 episode | steps: 25 |score: 82.91\n",
      "2550 episode | steps: 93 |score: 83.89\n",
      "2560 episode | steps: 108 |score: 83.99\n",
      "2570 episode | steps: 104 |score: 85.20\n",
      "2580 episode | steps: 102 |score: 86.43\n",
      "2590 episode | steps: 36 |score: 84.80\n",
      "2600 episode | steps: 44 |score: 83.67\n",
      "2610 episode | steps: 99 |score: 83.58\n",
      "2620 episode | steps: 102 |score: 83.77\n",
      "2630 episode | steps: 88 |score: 84.81\n",
      "2640 episode | steps: 102 |score: 85.55\n",
      "2650 episode | steps: 91 |score: 86.72\n",
      "2660 episode | steps: 89 |score: 85.67\n",
      "2670 episode | steps: 29 |score: 85.57\n",
      "2680 episode | steps: 88 |score: 85.20\n",
      "2690 episode | steps: 106 |score: 84.19\n",
      "2700 episode | steps: 73 |score: 83.74\n",
      "2710 episode | steps: 104 |score: 82.98\n",
      "2720 episode | steps: 76 |score: 83.20\n",
      "2730 episode | steps: 72 |score: 83.25\n",
      "2740 episode | steps: 71 |score: 84.13\n",
      "2750 episode | steps: 82 |score: 83.70\n",
      "2760 episode | steps: 100 |score: 84.12\n",
      "2770 episode | steps: 97 |score: 85.72\n",
      "2780 episode | steps: 62 |score: 84.67\n",
      "2790 episode | steps: 23 |score: 84.04\n",
      "2800 episode | steps: 86 |score: 83.29\n",
      "2810 episode | steps: 31 |score: 82.75\n",
      "2820 episode | steps: 91 |score: 82.51\n",
      "2830 episode | steps: 95 |score: 83.33\n",
      "2840 episode | steps: 74 |score: 83.40\n",
      "2850 episode | steps: 94 |score: 83.97\n",
      "2860 episode | steps: 100 |score: 85.09\n",
      "2870 episode | steps: 104 |score: 86.24\n",
      "2880 episode | steps: 93 |score: 85.26\n",
      "2890 episode | steps: 39 |score: 84.89\n",
      "2900 episode | steps: 102 |score: 84.24\n",
      "2910 episode | steps: 104 |score: 84.51\n",
      "2920 episode | steps: 105 |score: 85.48\n",
      "2930 episode | steps: 94 |score: 85.11\n",
      "2940 episode | steps: 96 |score: 85.17\n",
      "2950 episode | steps: 40 |score: 85.68\n",
      "2960 episode | steps: 105 |score: 86.06\n",
      "2970 episode | steps: 34 |score: 85.68\n",
      "2980 episode | steps: 101 |score: 86.26\n",
      "2990 episode | steps: 88 |score: 87.56\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env.seed(500)\n",
    "torch.manual_seed(500)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('state size:', num_inputs)\n",
    "print('action size:', num_actions)\n",
    "\n",
    "net = QNet(num_inputs, num_actions)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "net.to(device)\n",
    "net.train()\n",
    "running_score = 0\n",
    "# steps = 0\n",
    "loss = 0\n",
    "\n",
    "# 1: \n",
    "for e in range(3000):\n",
    "    done = False\n",
    "    memory = Memory()\n",
    "\n",
    "    steps = 0\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    state = state.unsqueeze(0)\n",
    "\n",
    "    # 2: 신경망에서 각 상태에 대해 확률의 평균값을 얻기\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        #현재 State를 가지고 다음에 취하게 될 action을 확률로 추정한 후 그 확률을 가지고 random하게 다음 action을 선정\n",
    "        # next_state(obs)는 아래와 같이 구성\n",
    "        # * 수평위치\n",
    "        # * 카트의 속도\n",
    "        # * 막대의 각도\n",
    "        # * 막대의 각속도\n",
    "        action = net.get_action(state)\n",
    "        \n",
    "        # 추정한 action을 가지고 수행 \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = torch.Tensor(next_state)\n",
    "        # 1차원 -> 2차원으로 확장 (행 개념 ) (4,) => (1,4)\n",
    "        next_state = next_state.unsqueeze(0)\n",
    "\n",
    "        mask = 0 if done else 1\n",
    "        reward = reward if not done or score == 499 else -1\n",
    "\n",
    "        # 0과 1중 action이 취해진 항목에 대해 1로 값을 반영\n",
    "        action_one_hot = torch.zeros(2)\n",
    "        action_one_hot[action] = 1\n",
    "        # 메모리에  현재 state, 다음 state, 현재 state에서 취한 행동, 보상, mask를 기록\n",
    "        memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "        # 학습 상황을 보기 위해  점수(== 보상) 출력\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    # 폴이 쓰러져서 한 eposiode가 끝난 경우, 그에 대한 학습을 수행\n",
    "    loss = QNet.train_model(net, memory.sample(), optimizer)\n",
    "\n",
    "    score = score if score == 500.0 else score + 1\n",
    "    running_score = 0.99 * running_score + 0.01 * score\n",
    "    if e % log_interval == 0:\n",
    "        print('{} episode | steps: {} |score: {:.2f}'.format(\n",
    "            e, steps ,running_score))\n",
    "#         writer.add_scalar('log/score', float(running_score), e)\n",
    "#         writer.add_scalar('log/loss', float(loss), e)\n",
    "\n",
    "    if running_score > goal_score:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.7 마르코프 결정 과정\n",
    "\n",
    "### 마르코프 연쇄 (Markov Chain)\n",
    "정해진 개수의 상태(s; state)를 가지고 있으며, 각 스텝마다 한 상태에서 다른 상태로 랜덤하게 전이 된다. 상태 $s$에서 상태 $s^\\prime$으로 전이하기 위한 확률은 고정되어 있으며, 시스템에 메모리가 없으므로 **과거 상태에는 상관없이 $(s,s^\\prime)$쌍에 만 의존하여 연쇄작용이 발생한다(== 마르코프 성질[markov property]을 가진다고도 한다).**\n",
    "\n",
    "이를 도식화한 그림은 아래와 같으며, 특정 state에서 자기 자신으로 전이하는 확률이 1.0인 경우에는 종료 상태(absorbing state)라고 한다.\n",
    "\n",
    "![markov_chain](../../img/markov_chain.png)\n",
    "<div style=\"margin-top:10px;font-size:16px;font-weight:bold\" align=\"center\">마르코프 연쇄 도식</div>\n",
    "\n",
    "### <a href=\"https://apps.dtic.mil/sti/pdfs/AD0606367.pdf\">마르코프 결정 과정 (Markov Decision Process; MDP)</a>\n",
    "\n",
    "MDP는 마르코프 연쇄와 비슷하지만 약간 다른점이 있다.\n",
    "* 각 스텝에서 에이전트는 여러 가능한 행동 중 하나를 선택한다.\n",
    "* 전이 확률은 선택된 행동에 따라 달라진다.\n",
    "* 어떤 상태 전이는 보상(양수 또는 음수)를 반환한다.\n",
    "* 에이전트의 목적은 시간이 지남에 따라 보상을 극대화 하기 위한 정책을 찾는 것\n",
    "\n",
    "![markov_decision_process](../../img/markov_decision_process.jpg)\n",
    "<div style=\"margin-top:30px;font-size:16px;font-weight:bold\" align=\"center\">마르코프 결정 과정 도식</div>\n",
    "\n",
    "어떤 상태의 최적의 상태가치(state value) $V*(s)$를 추정하는 방법을 MDP를 제안한 Richard Bellman은 찾아냈다. 이는 에이전트가 상태 s에 도달한 후 최적으로 행동한다고 가정하고 평균적으로 기대할 수 있는 할인된 미래 보상의 합을 말한다.\n",
    "\n",
    "<div style=\"margin:15px\" align=\"center\">\n",
    "$V^*(s) = \\underset{a}{\\max}\\sum T(s,a,s^\\prime)[R(s,a,s^\\prime) + \\gamma*V^*(s^\\prime)]$\n",
    "</div>\n",
    "<div style=\"margin-top:10px;font-size:16px;font-weight:bold\" align=\"center\">벨먼 최적 방정식 </div>\n",
    "\n",
    "* $T(s,a,s^\\prime)$는 에이전트가 행동 a를 선택했을 때 상태 s에서 상태 $s^\\prime$으로 전이될 확률\n",
    "* $R(s,a,s^\\prime)$는 에이전트가 행동 a를 선택해서 상태 s에서 상태 $s^\\prime$로 이동되었을 떄 에이전트가 받을 수 있는 보상\n",
    "* $\\gamma$는 할인 계수\n",
    "\n",
    "이를 통해 최적의 값을 얻어내기 위해서 가능한 경로들에 대해 0으로 초기화 한후, 가치 반복 알고리즘을 통해 업데이트를 수행한다.\n",
    "\n",
    "<div style=\"margin:15px\" align=\"center\">\n",
    "$V_{k+1}(s) = \\underset{a}{\\max}\\underset{s^\\prime}{\\sum} T(s,a,s^\\prime)[R(s,a,s^\\prime) + \\gamma*V_k(s^\\prime)]$\n",
    "</div>\n",
    "<div style=\"margin-top:10px;font-size:16px;font-weight:bold\" align=\"center\">가치 반복 알고리즘 </div>\n",
    "\n",
    "* $V_k(s)$는 알고리즘의 k번째 반복에서 상태 s의 추정가치를 나타냄\n",
    "\n",
    "최적의 상태 가치를 아는 것은 정책을 평가 할 때 유용하지만, 에이전트를 위한 최적의 정책을 알려주지 않는다. 따라서 벨먼은 이를 보완한 Q-value라고 부르는 최적의 상태-행동가치를 추정할 수 있는 매우 비슷한 알고리즘을 발견했다.\n",
    "\n",
    "상태-행동(s,a) 쌍에 대한 최적의 Q-가치인 $Q*(s,a)$는 에이전트가 상태 s에 도달해서 행동 a를 선택한 후 이 행동의 결과를 얻기 전에 평균적으로 기대할 수 있는 할인된 미래 보상의 합이다.\n",
    "\n",
    "<div style=\"margin:15px\" align=\"center\">\n",
    "$Q_{k+1}(s,a) = \\underset{s^\\prime}{\\sum} T(s,a,s^\\prime)[R(s,a,s^\\prime) + \\gamma*\\underset{a^\\prime}{\\max}Q_k(s^\\prime,a^\\prime)]$\n",
    "</div>\n",
    "<div style=\"margin-top:10px;font-size:16px;font-weight:bold\" align=\"center\">Q-가치 반복 알고리즘 </div>\n",
    "\n",
    "최적의 Q-가치를 구하게 되면, 최적의 정책인 $\\pi*(s)$를 정의하는 것은 아래와 같다.\n",
    "\n",
    "$\\pi^*(s)=\\underset{a}{argmax}Q^*(s,a)$\n",
    "\n",
    "마지막으로 얻은 Q-가치에 대해 state(row)들에 대한 max값을 return 하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    # state 0에서 action 0, 1, 2이 state 0, 1, 2로 전이될 확률\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    # state 1에서 action 0, 1, 2이 state 0, 1, 2로 전이될 확률\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "rewards = [\n",
    "    [[10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "# state 0, 1, 2에서 취할 수 있는 action\n",
    "possible_actions = [[0,1,2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = np.full((3,3),-np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            # s state에서 a action을 취하여 sp state로 전이되는 확률 * s state에서  a action을 취하여 sp state로 전이될 때의 부상 + 할인 계수 * 순회를 통해 얻는 sp state의 최대 Q value\n",
    "            Q_values[s,a] = np.sum([transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.77572778, 20.68076527, 16.74407361],\n",
       "       [ 0.99730695,        -inf,  1.05629796],\n",
       "       [       -inf, 53.74997271,        -inf]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
