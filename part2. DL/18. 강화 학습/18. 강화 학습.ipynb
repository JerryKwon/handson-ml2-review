{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. 강화 학습\n",
    "\n",
    "강화 학습(Reinforcement Learning)은 요즘 머신러닝에서 가장 흥미진진한 분야이자 가장 오래된 분야이다. 강화 학습이 현대에 다시 재조명을 받은 시기는 2013년에 딥마인드에서 시도한 <a href=\"https://arxiv.org/pdf/1312.5602.pdf?source=post_page---------------------------\">아타리게임들에 대해 수행된 연구</a>때문입니다. 이 연구에서 기계는 화면 픽셀에 대한 데이터만 입력으로 받고 <a href=\"https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf\">게임 규칙에 대한 어떤 사전 정보없이 대부분 사람을 능가하는 성과</a>를 냈다.\n",
    "\n",
    "딥마인드가 이러한 성과를 낼수 있던 이유는? 강화 학습 분야에 강력한 딥러닝을 적용했기 때문이다.\n",
    "\n",
    "18장에서는,\n",
    "* 강화 학습의 정의 및 활용 분야\n",
    "* 정책 그라디언트, 심층 Q-네트워크\n",
    "\n",
    "## 18.1. 보상을 최적화하기 위한 학습\n",
    "\n",
    "### 강화 학습의 구성요소\n",
    "\n",
    "* 에이전트: 인공지능 플레이어\n",
    "* 환경: 에이전트가 솔루션을 찾기 위한 무대\n",
    "* 행동: 에이전트가 환경 안에서 시행하는 상호작용\n",
    "* 보상: 에이전트의 행동에 따른 점수 혹은 결과\n",
    "\n",
    "위의 4가지 요소를 가지고 강화학습은 **'에이전트는 관측을 하고 주어진 환경에서 행동을 하고, 이에 대한 결과로 보상을 받는다'**라는 문장으로 요약할 수 있다. 에이전트는 환경 아래에서 시행착오를 겪으며 보상을 최대로 하는 방향으로 학습한다. 강화학습은 자율주행 자동차, 추천 시스템, 웹페이지에 광고 배치, 이미지 분류 시스템의 제어 등에 사용될 수 있다.\n",
    "\n",
    "## 18.2. 정책 탐색\n",
    "\n",
    "에이전트가 행동을 결정하기 위해 사용하는 알고리즘을 정책(policy)라고 한다. 아래의 그림과 같이 Agent가 위치한 상태를 입력으로 받고 행동을 출력하는 신경망이 정책이 될 수 있다.\n",
    "\n",
    "![RL_figure](../../img/RL_figure.jpg)\n",
    "\n",
    "### 강화 학습의 예시; 청소기\n",
    "\n",
    "* Agent: 30분 동안 수집한 먼지의 양을 보상으로 받는 로봇 진공청소기\n",
    "* 정책: 매 초마다 p의 확률로 전진 or (1-p)의 확률로 왼쪽 또는 오른쪽으로 랜덤하게 회전; 회전의 각도는 -r과 +r 사이의 랜덤한 각도\n",
    "\n",
    "**어떻게 훈련할 수 있을까?(정책탐색; Policy Search)**\n",
    "1. 무작위 방식: 정책 파라미터들에 대해 무작위로 시행을 수행하고 성능이 좋은 조합을 선택\n",
    "2. 유전 알고리즘(Genetic Algorithm): 1세대 정책 100개를 랜덤하여 생성하고, 하위 80개의 정책을 drop. 남은 20개를 활용하여 자식 정책 4개를 생성한다. 자식 정책 4개는 복사된 부모의 정책과 약간의 무작위 성을 설정한 것.\n",
    "3. 정책 그라디언트(Policy Gradient): 정책 파라미터에 대한 보상의 그라디언트를 평가하여 높은 보상의 방향을 따르는 그라디언트로 파라미터를 수정하는 최적화 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3. OpenAI 짐\n",
    "\n",
    "강화 학습 에이전트 훈련을 위한 최소한의 시뮬레이션 환경을 제공하는 패키지\n",
    "\n",
    "간단하게 확인해 볼 환경은 CartPole이라는 아타리의 게임 중 기울어지는 막대를 세우는 게임이다. \n",
    "\n",
    "CartPole 환경에서 return되는 관측값은 아래와 같이 구성된다.\n",
    "\n",
    "[수평 위치(0.0=중앙), 카트의 속도(양수=우측; 음수=좌측), 막대의 각도(0.0=수직), 막대의 각속도(양수=시계방향; 음수=반시계방향)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02556377, -0.03921013,  0.03143846,  0.03229114])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01264813,  0.15074345, -0.01487873, -0.34635302])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "totals= np.array(totals)\n",
    "totals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:42.51; std:8.4336; min:24.0; max:67.0\n"
     ]
    }
   ],
   "source": [
    "rewards_mean = np.mean(totals)\n",
    "rewards_std = np.std(totals)\n",
    "rewards_min = np.min(totals)\n",
    "rewards_max = np.max(totals)\n",
    "\n",
    "print(\"mean:{:.2f}; std:{:.4f}; min:{}; max:{}\".format(rewards_mean,rewards_std,rewards_min,rewards_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 신경망 정책\n",
    "\n",
    "신경망 정책에서는 관측값을 통해 특정 결과에 대한 확률을 추정한다. 그리고 추정환 확률을 기반으로 랜덤으로 행동하도록 선택한다. 그렇다면 여기서 왜 '랜덤'하게 행동하도록 선택할까? 그 이유는 에이전트가 새로운 행동을 탐험하고 잘 할 수 있는 행동을 활용하는 행동을 유도하기 위함이다. \n",
    "\n",
    "일반적으로 각 관측은 환경에 대한 완전한 상태를 갖고 있기 때문에 과거 관측값에 대한 고려가 필요없다(e.g) CartPole). 그러나 관측에 잡음이 있는 경우에는 가능성있는 현재 상태의 추정을 위해 지난 관측 몇 개를 사용하는 것이 좋다.\n",
    "\n",
    "## 18.5. 행동 평가: 신용 할당 문제\n",
    "\n",
    "강화 학습에서는 일반적인 지도학습과는 달리 학습에 대한 평가 시, 실제 값 또는 Label이 주어지지 않는다. 다시 말해, 학습을 평가하는데 있어 사용되는 지표는 행동으로 주어지는 보상(reward)밖에 없다는 것이다. 그렇다면 에이전트가 수행한 각 행동에 대해 어떤 것이 좋고 나쁨을 구별할 수 있을까? 이는 신용 할당 문제(credit assignment problem)이라고 불린다.\n",
    "\n",
    "위의 문제를 해결하기 위해 행동이 일어난 후 각 단계마다 할인 계수(discount factor; $\\gamma$)를 적용한 보상을 모두 합하여 행동을 평가하는 것입니다. 이렇게 보상이 모두 합쳐진 값을 대가(return)이라고 부릅니다.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top:20px\">$0\\le\\gamma(할인 계수; discount factor)\\ge1$</div>\n",
    "<div align=\"center\" style=\"margin-top:10px\"><b>할인 계수 범위</b></div>\n",
    "\n",
    "할인 계수는 0과 1사이의 값으로 구성되며, 값이 클 수록 미래 시점에 주어지는 보상에 대해 우선순위롤 높게주는 것이고, 값이 낮을 수록 현재 시점에 주어지는 보상에 우선순위를 높게 주는 것이다. 일반적으로 $0.9~0.99$사이의 값을 준다.\n",
    "\n",
    "e.g) 0.95: 13step 이후의 보상 50% 할인; 0.99: 69step 이후의 보상 50% 할인\n",
    "\n",
    "위의 원리로 수행하게 된다면, 좋은 행동 후에 나쁜 행동이 이어져 낮은 대가를 받을 수 있다. 하지만, 평균적으로 다른 가능한 행동과 비교하여 각 행동이 얼마나 좋은지 혹은 나쁜지를 추정해야 한다. 이를 행동이익이라고 부르며, 많은 에피소드를 실행하여 모든 행동의 대가를 정규화해야 한다.\n",
    "\n",
    "## 18.6. 정책 그라디언트\n",
    "\n",
    "### REINFORCEMENT 알고리즘\n",
    "\n",
    "참고\n",
    "\n",
    "https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0\n",
    "\n",
    "https://github.com/g6ling/Reinforcement-Learning-Pytorch-Cartpole/tree/master/PG/1-REINFORCE\n",
    "\n",
    "https://wonseokjung.github.io/page5/\n",
    "\n",
    "1. 먼저 신경망 정책이 여러 번에 걸쳐 게임을 플레이하고 매 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그라디언트를 계산합니다. 하지만 그라디언트를 적용하지는 않는다.\n",
    "2. 에피소드를 몇 번 실행한 다음, 각 행동의 이익을 계산한다.\n",
    "3. 한 행동의 이익이 양수이면, 이 행동이 좋은 것임을 의미하므로 미래에 선택될 가능성이 높도록 앞서 계산한 그라디언트를 적용합니다. 그러나 행동이익이 음수이면 이 행동이 나쁜 것임을 의미하므로 미래에 이 행동이 덜 선택되도록 반대의 그라디언트를 적용합니다. 이는 각 그라디언트 벡터와 그에 상응하는 행동의 이익을 곱하면 됩니다.\n",
    "4. 마지막으로 모든 결과 그라디언트 벡터를 평균 내어 경사 하강법 스텝을 수행합니다.\n",
    "\n",
    "![sudo_REINFORCMENT](../../img/sudo_REINFORCEMENT.png)\n",
    "\n",
    "### 주요 파라미터\n",
    "* step size\n",
    "* distcount rate\n",
    "* batch size\n",
    "* max epsiodes\n",
    "\n",
    "### 정책 손실 $L(\\theta)$\n",
    " 신경망을 통해 도출되는 값은 확률분포를 따른다. 그렇기 때문에 $\\pi(a | s,\\theta)$는 신경망에서 각 상태에 대해 확률의 평균값을 얻기 위함이다. 그리고 확률의 평균 값을 할인 계수로 곱하여 신경망의 기대값을 계산한다.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "goal_score = 200\n",
    "log_interval = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "\n",
    "    def sample(self):\n",
    "        memory = self.memory\n",
    "        return Transition(*zip(*memory)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# from config import gamma\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(QNet, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc_2 = nn.Linear(128, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc_1(input))\n",
    "        policy = F.softmax(self.fc_2(x))\n",
    "        return policy\n",
    "\n",
    "    @classmethod\n",
    "    def train_model(cls, net, transitions, optimizer):\n",
    "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "\n",
    "        running_return = 0\n",
    "        # 각 state 별 미래가치를 현재가치로 할인한 보상의 값\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
    "            returns[t] = running_return\n",
    "        \n",
    "        policies = net(states)\n",
    "        policies = policies.view(-1, net.num_outputs)\n",
    "\n",
    "        log_policies = (torch.log(policies) * actions.detach()).sum(dim=1)\n",
    "\n",
    "        loss = (-log_policies * returns).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_action(self, input):\n",
    "        policy = self.forward(input)\n",
    "        policy = policy[0].data.numpy()\n",
    "\n",
    "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4\n",
      "action size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode | score: 0.50\n",
      "10 episode | score: 2.41\n",
      "20 episode | score: 3.68\n",
      "30 episode | score: 5.33\n",
      "40 episode | score: 7.04\n",
      "50 episode | score: 8.76\n",
      "60 episode | score: 10.41\n",
      "70 episode | score: 12.03\n",
      "80 episode | score: 13.79\n",
      "90 episode | score: 16.44\n",
      "100 episode | score: 18.48\n",
      "110 episode | score: 19.17\n",
      "120 episode | score: 20.06\n",
      "130 episode | score: 21.21\n",
      "140 episode | score: 21.75\n",
      "150 episode | score: 22.58\n",
      "160 episode | score: 24.24\n",
      "170 episode | score: 26.03\n",
      "180 episode | score: 27.73\n",
      "190 episode | score: 28.64\n",
      "200 episode | score: 30.87\n",
      "210 episode | score: 31.73\n",
      "220 episode | score: 31.68\n",
      "230 episode | score: 32.91\n",
      "240 episode | score: 33.82\n",
      "250 episode | score: 34.99\n",
      "260 episode | score: 35.34\n",
      "270 episode | score: 36.61\n",
      "280 episode | score: 36.99\n",
      "290 episode | score: 37.33\n",
      "300 episode | score: 37.40\n",
      "310 episode | score: 37.74\n",
      "320 episode | score: 38.21\n",
      "330 episode | score: 37.66\n",
      "340 episode | score: 37.19\n",
      "350 episode | score: 38.90\n",
      "360 episode | score: 39.65\n",
      "370 episode | score: 39.59\n",
      "380 episode | score: 39.79\n",
      "390 episode | score: 39.70\n",
      "400 episode | score: 40.56\n",
      "410 episode | score: 40.38\n",
      "420 episode | score: 41.27\n",
      "430 episode | score: 41.14\n",
      "440 episode | score: 42.04\n",
      "450 episode | score: 42.41\n",
      "460 episode | score: 43.22\n",
      "470 episode | score: 43.29\n",
      "480 episode | score: 43.69\n",
      "490 episode | score: 43.79\n",
      "500 episode | score: 44.33\n",
      "510 episode | score: 44.14\n",
      "520 episode | score: 43.80\n",
      "530 episode | score: 43.76\n",
      "540 episode | score: 44.62\n",
      "550 episode | score: 45.11\n",
      "560 episode | score: 45.78\n",
      "570 episode | score: 45.29\n",
      "580 episode | score: 46.88\n",
      "590 episode | score: 45.44\n",
      "600 episode | score: 46.27\n",
      "610 episode | score: 45.32\n",
      "620 episode | score: 44.69\n",
      "630 episode | score: 45.51\n",
      "640 episode | score: 46.82\n",
      "650 episode | score: 47.36\n",
      "660 episode | score: 47.97\n",
      "670 episode | score: 47.95\n",
      "680 episode | score: 48.10\n",
      "690 episode | score: 48.01\n",
      "700 episode | score: 47.45\n",
      "710 episode | score: 47.00\n",
      "720 episode | score: 46.97\n",
      "730 episode | score: 46.70\n",
      "740 episode | score: 46.43\n",
      "750 episode | score: 45.99\n",
      "760 episode | score: 46.53\n",
      "770 episode | score: 47.16\n",
      "780 episode | score: 46.83\n",
      "790 episode | score: 47.12\n",
      "800 episode | score: 47.23\n",
      "810 episode | score: 47.25\n",
      "820 episode | score: 47.58\n",
      "830 episode | score: 47.19\n",
      "840 episode | score: 46.74\n",
      "850 episode | score: 46.57\n",
      "860 episode | score: 46.14\n",
      "870 episode | score: 46.16\n",
      "880 episode | score: 46.10\n",
      "890 episode | score: 45.42\n",
      "900 episode | score: 45.78\n",
      "910 episode | score: 45.73\n",
      "920 episode | score: 46.36\n",
      "930 episode | score: 45.89\n",
      "940 episode | score: 47.02\n",
      "950 episode | score: 46.84\n",
      "960 episode | score: 47.40\n",
      "970 episode | score: 48.15\n",
      "980 episode | score: 47.95\n",
      "990 episode | score: 47.30\n",
      "1000 episode | score: 47.44\n",
      "1010 episode | score: 48.17\n",
      "1020 episode | score: 47.91\n",
      "1030 episode | score: 48.08\n",
      "1040 episode | score: 48.36\n",
      "1050 episode | score: 48.59\n",
      "1060 episode | score: 48.40\n",
      "1070 episode | score: 49.21\n",
      "1080 episode | score: 49.89\n",
      "1090 episode | score: 49.76\n",
      "1100 episode | score: 50.94\n",
      "1110 episode | score: 51.04\n",
      "1120 episode | score: 50.58\n",
      "1130 episode | score: 49.97\n",
      "1140 episode | score: 50.42\n",
      "1150 episode | score: 50.84\n",
      "1160 episode | score: 51.29\n",
      "1170 episode | score: 51.88\n",
      "1180 episode | score: 51.06\n",
      "1190 episode | score: 50.93\n",
      "1200 episode | score: 51.27\n",
      "1210 episode | score: 51.89\n",
      "1220 episode | score: 51.63\n",
      "1230 episode | score: 52.20\n",
      "1240 episode | score: 53.21\n",
      "1250 episode | score: 54.30\n",
      "1260 episode | score: 53.79\n",
      "1270 episode | score: 53.29\n",
      "1280 episode | score: 52.99\n",
      "1290 episode | score: 52.43\n",
      "1300 episode | score: 52.20\n",
      "1310 episode | score: 52.15\n",
      "1320 episode | score: 52.57\n",
      "1330 episode | score: 52.93\n",
      "1340 episode | score: 53.06\n",
      "1350 episode | score: 53.58\n",
      "1360 episode | score: 54.03\n",
      "1370 episode | score: 53.45\n",
      "1380 episode | score: 52.89\n",
      "1390 episode | score: 52.74\n",
      "1400 episode | score: 52.68\n",
      "1410 episode | score: 52.61\n",
      "1420 episode | score: 53.67\n",
      "1430 episode | score: 53.50\n",
      "1440 episode | score: 53.99\n",
      "1450 episode | score: 56.64\n",
      "1460 episode | score: 57.74\n",
      "1470 episode | score: 59.81\n",
      "1480 episode | score: 59.45\n",
      "1490 episode | score: 60.30\n",
      "1500 episode | score: 60.28\n",
      "1510 episode | score: 59.04\n",
      "1520 episode | score: 58.60\n",
      "1530 episode | score: 60.71\n",
      "1540 episode | score: 59.79\n",
      "1550 episode | score: 61.26\n",
      "1560 episode | score: 62.06\n",
      "1570 episode | score: 61.63\n",
      "1580 episode | score: 62.50\n",
      "1590 episode | score: 62.94\n",
      "1600 episode | score: 63.09\n",
      "1610 episode | score: 63.87\n",
      "1620 episode | score: 64.88\n",
      "1630 episode | score: 64.95\n",
      "1640 episode | score: 65.42\n",
      "1650 episode | score: 66.04\n",
      "1660 episode | score: 66.68\n",
      "1670 episode | score: 66.26\n",
      "1680 episode | score: 67.05\n",
      "1690 episode | score: 69.68\n",
      "1700 episode | score: 71.56\n",
      "1710 episode | score: 72.82\n",
      "1720 episode | score: 74.18\n",
      "1730 episode | score: 74.35\n",
      "1740 episode | score: 75.26\n",
      "1750 episode | score: 76.94\n",
      "1760 episode | score: 77.84\n",
      "1770 episode | score: 79.09\n",
      "1780 episode | score: 79.22\n",
      "1790 episode | score: 79.55\n",
      "1800 episode | score: 79.78\n",
      "1810 episode | score: 81.20\n",
      "1820 episode | score: 80.51\n",
      "1830 episode | score: 79.87\n",
      "1840 episode | score: 79.75\n",
      "1850 episode | score: 79.41\n",
      "1860 episode | score: 80.25\n",
      "1870 episode | score: 78.57\n",
      "1880 episode | score: 79.41\n",
      "1890 episode | score: 80.29\n",
      "1900 episode | score: 80.23\n",
      "1910 episode | score: 81.34\n",
      "1920 episode | score: 82.42\n",
      "1930 episode | score: 84.33\n",
      "1940 episode | score: 85.92\n",
      "1950 episode | score: 86.77\n",
      "1960 episode | score: 86.22\n",
      "1970 episode | score: 87.07\n",
      "1980 episode | score: 88.71\n",
      "1990 episode | score: 89.29\n",
      "2000 episode | score: 89.58\n",
      "2010 episode | score: 90.49\n",
      "2020 episode | score: 92.03\n",
      "2030 episode | score: 93.54\n",
      "2040 episode | score: 93.88\n",
      "2050 episode | score: 95.16\n",
      "2060 episode | score: 96.25\n",
      "2070 episode | score: 97.47\n",
      "2080 episode | score: 96.82\n",
      "2090 episode | score: 97.50\n",
      "2100 episode | score: 96.38\n",
      "2110 episode | score: 95.58\n",
      "2120 episode | score: 95.73\n",
      "2130 episode | score: 96.17\n",
      "2140 episode | score: 94.99\n",
      "2150 episode | score: 95.16\n",
      "2160 episode | score: 95.34\n",
      "2170 episode | score: 94.63\n",
      "2180 episode | score: 95.85\n",
      "2190 episode | score: 96.82\n",
      "2200 episode | score: 95.83\n",
      "2210 episode | score: 95.24\n",
      "2220 episode | score: 94.81\n",
      "2230 episode | score: 94.71\n",
      "2240 episode | score: 94.31\n",
      "2250 episode | score: 92.92\n",
      "2260 episode | score: 93.05\n",
      "2270 episode | score: 93.11\n",
      "2280 episode | score: 93.49\n",
      "2290 episode | score: 93.68\n",
      "2300 episode | score: 95.15\n",
      "2310 episode | score: 96.01\n",
      "2320 episode | score: 96.22\n",
      "2330 episode | score: 96.76\n",
      "2340 episode | score: 96.53\n",
      "2350 episode | score: 96.76\n",
      "2360 episode | score: 94.77\n",
      "2370 episode | score: 96.12\n",
      "2380 episode | score: 95.49\n",
      "2390 episode | score: 96.28\n",
      "2400 episode | score: 96.61\n",
      "2410 episode | score: 96.41\n",
      "2420 episode | score: 96.82\n",
      "2430 episode | score: 97.46\n",
      "2440 episode | score: 97.79\n",
      "2450 episode | score: 99.16\n",
      "2460 episode | score: 99.16\n",
      "2470 episode | score: 99.65\n",
      "2480 episode | score: 100.21\n",
      "2490 episode | score: 100.74\n",
      "2500 episode | score: 101.73\n",
      "2510 episode | score: 102.23\n",
      "2520 episode | score: 102.54\n",
      "2530 episode | score: 103.54\n",
      "2540 episode | score: 103.93\n",
      "2550 episode | score: 104.15\n",
      "2560 episode | score: 104.29\n",
      "2570 episode | score: 104.19\n",
      "2580 episode | score: 103.26\n",
      "2590 episode | score: 103.34\n",
      "2600 episode | score: 103.91\n",
      "2610 episode | score: 103.52\n",
      "2620 episode | score: 103.30\n",
      "2630 episode | score: 103.11\n",
      "2640 episode | score: 103.70\n",
      "2650 episode | score: 103.32\n",
      "2660 episode | score: 103.82\n",
      "2670 episode | score: 104.08\n",
      "2680 episode | score: 103.58\n",
      "2690 episode | score: 103.79\n",
      "2700 episode | score: 103.75\n",
      "2710 episode | score: 104.65\n",
      "2720 episode | score: 105.36\n",
      "2730 episode | score: 105.36\n",
      "2740 episode | score: 106.28\n",
      "2750 episode | score: 106.88\n",
      "2760 episode | score: 107.51\n",
      "2770 episode | score: 108.33\n",
      "2780 episode | score: 109.31\n",
      "2790 episode | score: 109.71\n",
      "2800 episode | score: 109.92\n",
      "2810 episode | score: 109.87\n",
      "2820 episode | score: 110.22\n",
      "2830 episode | score: 110.36\n",
      "2840 episode | score: 110.87\n",
      "2850 episode | score: 110.03\n",
      "2860 episode | score: 109.98\n",
      "2870 episode | score: 110.11\n",
      "2880 episode | score: 110.61\n",
      "2890 episode | score: 110.56\n",
      "2900 episode | score: 110.66\n",
      "2910 episode | score: 110.39\n",
      "2920 episode | score: 109.36\n",
      "2930 episode | score: 108.83\n",
      "2940 episode | score: 108.82\n",
      "2950 episode | score: 108.74\n",
      "2960 episode | score: 108.09\n",
      "2970 episode | score: 108.57\n",
      "2980 episode | score: 107.71\n",
      "2990 episode | score: 107.24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from model import QNet\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "# from memory import Memory\n",
    "# from config import env_name, goal_score, log_interval, device, lr, gamma\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(500)\n",
    "torch.manual_seed(500)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('state size:', num_inputs)\n",
    "print('action size:', num_actions)\n",
    "\n",
    "net = QNet(num_inputs, num_actions)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "# writer = SummaryWriter('logs')\n",
    "\n",
    "net.to(device)\n",
    "net.train()\n",
    "running_score = 0\n",
    "steps = 0\n",
    "loss = 0\n",
    "\n",
    "for e in range(3000):\n",
    "    done = False\n",
    "    memory = Memory()\n",
    "\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    state = state.unsqueeze(0)\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        action = net.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        next_state = torch.Tensor(next_state)\n",
    "        next_state = next_state.unsqueeze(0)\n",
    "\n",
    "        mask = 0 if done else 1\n",
    "        reward = reward if not done or score == 499 else -1\n",
    "\n",
    "        action_one_hot = torch.zeros(2)\n",
    "        action_one_hot[action] = 1\n",
    "        memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    loss = QNet.train_model(net, memory.sample(), optimizer)\n",
    "\n",
    "\n",
    "    score = score if score == 500.0 else score + 1\n",
    "    running_score = 0.99 * running_score + 0.01 * score\n",
    "    if e % log_interval == 0:\n",
    "        print('{} episode | score: {:.2f}'.format(\n",
    "            e, running_score))\n",
    "#         writer.add_scalar('log/score', float(running_score), e)\n",
    "#         writer.add_scalar('log/loss', float(loss), e)\n",
    "\n",
    "    if running_score > goal_score:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(obs,dtype=torch.float,requires_grad=True)\n",
    "left_proba = model(input_tensor)\n",
    "action = (random.random() > left_proba).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "left_proba.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
