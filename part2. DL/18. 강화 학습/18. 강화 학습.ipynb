{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. 강화 학습\n",
    "\n",
    "강화 학습(Reinforcement Learning)은 요즘 머신러닝에서 가장 흥미진진한 분야이자 가장 오래된 분야이다. 강화 학습이 현대에 다시 재조명을 받은 시기는 2013년에 딥마인드에서 시도한 <a href=\"https://arxiv.org/pdf/1312.5602.pdf?source=post_page---------------------------\">아타리게임들에 대해 수행된 연구</a>때문입니다. 이 연구에서 기계는 화면 픽셀에 대한 데이터만 입력으로 받고 <a href=\"https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf\">게임 규칙에 대한 어떤 사전 정보없이 대부분 사람을 능가하는 성과</a>를 냈다.\n",
    "\n",
    "딥마인드가 이러한 성과를 낼수 있던 이유는? 강화 학습 분야에 강력한 딥러닝을 적용했기 때문이다.\n",
    "\n",
    "18장에서는,\n",
    "* 강화 학습의 정의 및 활용 분야\n",
    "* 정책 그라디언트, 심층 Q-네트워크\n",
    "\n",
    "## 18.1. 보상을 최적화하기 위한 학습\n",
    "\n",
    "### 강화 학습의 구성요소\n",
    "\n",
    "* 에이전트: 인공지능 플레이어\n",
    "* 환경: 에이전트가 솔루션을 찾기 위한 무대\n",
    "* 행동: 에이전트가 환경 안에서 시행하는 상호작용\n",
    "* 보상: 에이전트의 행동에 따른 점수 혹은 결과\n",
    "\n",
    "위의 4가지 요소를 가지고 강화학습은 **'에이전트는 관측을 하고 주어진 환경에서 행동을 하고, 이에 대한 결과로 보상을 받는다'**라는 문장으로 요약할 수 있다. 에이전트는 환경 아래에서 시행착오를 겪으며 보상을 최대로 하는 방향으로 학습한다. 강화학습은 자율주행 자동차, 추천 시스템, 웹페이지에 광고 배치, 이미지 분류 시스템의 제어 등에 사용될 수 있다.\n",
    "\n",
    "## 18.2. 정책 탐색\n",
    "\n",
    "에이전트가 행동을 결정하기 위해 사용하는 알고리즘을 정책(policy)라고 한다. 아래의 그림과 같이 Agent가 위치한 상태를 입력으로 받고 행동을 출력하는 신경망이 정책이 될 수 있다.\n",
    "\n",
    "![RL_figure](../../img/RL_figure.jpg)\n",
    "\n",
    "### 강화 학습의 예시; 청소기\n",
    "\n",
    "* Agent: 30분 동안 수집한 먼지의 양을 보상으로 받는 로봇 진공청소기\n",
    "* 정책: 매 초마다 p의 확률로 전진 or (1-p)의 확률로 왼쪽 또는 오른쪽으로 랜덤하게 회전; 회전의 각도는 -r과 +r 사이의 랜덤한 각도\n",
    "\n",
    "**어떻게 훈련할 수 있을까?(정책탐색; Policy Search)**\n",
    "1. 무작위 방식: 정책 파라미터들에 대해 무작위로 시행을 수행하고 성능이 좋은 조합을 선택\n",
    "2. 유전 알고리즘(Genetic Algorithm): 1세대 정책 100개를 랜덤하여 생성하고, 하위 80개의 정책을 drop. 남은 20개를 활용하여 자식 정책 4개를 생성한다. 자식 정책 4개는 복사된 부모의 정책과 약간의 무작위 성을 설정한 것.\n",
    "3. 정책 그라디언트(Policy Gradient): 정책 파라미터에 대한 보상의 그라디언트를 평가하여 높은 보상의 방향을 따르는 그라디언트로 파라미터를 수정하는 최적화 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3. OpenAI 짐\n",
    "\n",
    "강화 학습 에이전트 훈련을 위한 최소한의 시뮬레이션 환경을 제공하는 패키지\n",
    "\n",
    "간단하게 확인해 볼 환경은 CartPole이라는 아타리의 게임 중 기울어지는 막대를 세우는 게임이다. \n",
    "\n",
    "CartPole 환경에서 return되는 관측값은 아래와 같이 구성된다.\n",
    "\n",
    "[수평 위치(0.0=중앙), 카트의 속도(양수=우측; 음수=좌측), 막대의 각도(0.0=수직), 막대의 각속도(양수=시계방향; 음수=반시계방향)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0159937 , 0.00347802, 0.01264561, 0.0356175 ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01606326,  0.19841637,  0.01335796, -0.25304894])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "totals= np.array(totals)\n",
    "totals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:42.46; std:8.8997; min:24.0; max:72.0\n"
     ]
    }
   ],
   "source": [
    "rewards_mean = np.mean(totals)\n",
    "rewards_std = np.std(totals)\n",
    "rewards_min = np.min(totals)\n",
    "rewards_max = np.max(totals)\n",
    "\n",
    "print(\"mean:{:.2f}; std:{:.4f}; min:{}; max:{}\".format(rewards_mean,rewards_std,rewards_min,rewards_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 신경망 정책\n",
    "\n",
    "신경망 정책에서는 관측값을 통해 특정 결과에 대한 확률을 추정한다. 그리고 추정환 확률을 기반으로 랜덤으로 행동하도록 선택한다. 그렇다면 여기서 왜 '랜덤'하게 행동하도록 선택할까? 그 이유는 에이전트가 새로운 행동을 탐험하고 잘 할 수 있는 행동을 활용하는 행동을 유도하기 위함이다. \n",
    "\n",
    "일반적으로 각 관측은 환경에 대한 완전한 상태를 갖고 있기 때문에 과거 관측값에 대한 고려가 필요없다(e.g) CartPole). 그러나 관측에 잡음이 있는 경우에는 가능성있는 현재 상태의 추정을 위해 지난 관측 몇 개를 사용하는 것이 좋다.\n",
    "\n",
    "## 18.5. 행동 평가: 신용 할당 문제\n",
    "\n",
    "강화 학습에서는 일반적인 지도학습과는 달리 학습에 대한 평가 시, 실제 값 또는 Label이 주어지지 않는다. 다시 말해, 학습을 평가하는데 있어 사용되는 지표는 행동으로 주어지는 보상(reward)밖에 없다는 것이다. 그렇다면 에이전트가 수행한 각 행동에 대해 어떤 것이 좋고 나쁨을 구별할 수 있을까? 이는 신용 할당 문제(credit assignment problem)이라고 불린다.\n",
    "\n",
    "위의 문제를 해결하기 위해 행동이 일어난 후 각 단계마다 할인 계수(discount factor; $\\gamma$)를 적용한 보상을 모두 합하여 행동을 평가하는 것입니다. 이렇게 보상이 모두 합쳐진 값을 대가(return)이라고 부릅니다.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top:20px\">$0\\le\\gamma(할인 계수; discount factor)\\ge1$</div>\n",
    "<div align=\"center\" style=\"margin-top:10px\"><b>할인 계수 범위</b></div>\n",
    "\n",
    "할인 계수는 0과 1사이의 값으로 구성되며, 값이 클 수록 미래 시점에 주어지는 보상에 대해 우선순위롤 높게주는 것이고, 값이 낮을 수록 현재 시점에 주어지는 보상에 우선순위를 높게 주는 것이다. 일반적으로 $0.9~0.99$사이의 값을 준다.\n",
    "\n",
    "e.g) 0.95: 13step 이후의 보상 50% 할인; 0.99: 69step 이후의 보상 50% 할인\n",
    "\n",
    "위의 원리로 수행하게 된다면, 좋은 행동 후에 나쁜 행동이 이어져 낮은 대가를 받을 수 있다. 하지만, 평균적으로 다른 가능한 행동과 비교하여 각 행동이 얼마나 좋은지 혹은 나쁜지를 추정해야 한다. 이를 행동이익이라고 부르며, 많은 에피소드를 실행하여 모든 행동의 대가를 정규화해야 한다.\n",
    "\n",
    "## 18.6. 정책 그라디언트\n",
    "\n",
    "### REINFORCEMENT 알고리즘\n",
    "\n",
    "참고\n",
    "\n",
    "https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0\n",
    "\n",
    "https://github.com/g6ling/Reinforcement-Learning-Pytorch-Cartpole/tree/master/PG/1-REINFORCE\n",
    "\n",
    "https://wonseokjung.github.io/page5/\n",
    "\n",
    "1. 먼저 신경망 정책이 여러 번에 걸쳐 게임을 플레이하고 매 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그라디언트를 계산합니다. 하지만 그라디언트를 적용하지는 않는다.\n",
    "2. 에피소드를 몇 번 실행한 다음, 각 행동의 이익을 계산한다.\n",
    "3. 한 행동의 이익이 양수이면, 이 행동이 좋은 것임을 의미하므로 미래에 선택될 가능성이 높도록 앞서 계산한 그라디언트를 적용합니다. 그러나 행동이익이 음수이면 이 행동이 나쁜 것임을 의미하므로 미래에 이 행동이 덜 선택되도록 반대의 그라디언트를 적용합니다. 이는 각 그라디언트 벡터와 그에 상응하는 행동의 이익을 곱하면 됩니다.\n",
    "4. 마지막으로 모든 결과 그라디언트 벡터를 평균 내어 경사 하강법 스텝을 수행합니다.\n",
    "\n",
    "![sudo_REINFORCMENT](../../img/sudo_REINFORCEMENT.png)\n",
    "\n",
    "### 주요 파라미터\n",
    "* step size\n",
    "* distcount rate\n",
    "* batch size\n",
    "* max epsiodes\n",
    "\n",
    "### 정책 손실 $L(\\theta)$\n",
    " 신경망을 통해 도출되는 값은 확률분포를 따른다. 그렇기 때문에 $\\pi(a | s,\\theta)$는 신경망에서 각 상태에 대해 확률의 평균값을 얻기 위함이다. 그리고 확률의 평균 값을 할인 계수로 곱하여 신경망의 기대값을 계산한다.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "goal_score = 200\n",
    "log_interval = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "\n",
    "    def sample(self):\n",
    "        memory = self.memory\n",
    "        return Transition(*zip(*memory)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# from config import gamma\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(QNet, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc_2 = nn.Linear(128, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc_1(input))\n",
    "        policy = F.softmax(self.fc_2(x))\n",
    "        return policy\n",
    "\n",
    "    # 매 종료된 에피소드마다 각 행동을 평가하기 위한 함수\n",
    "    @classmethod\n",
    "    def train_model(cls, net, transitions, optimizer):\n",
    "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        \n",
    "        running_return = 0\n",
    "        \n",
    "#         tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "#          1.,  1.,  1.,  1.,  1., -1.])\n",
    "#         print(rewards)\n",
    "        \n",
    "        # 3: 각 state 별 미래가치를 현재가치로 할인한 보상의 값\n",
    "        # 각 state에서 추정한 action으로 얻은 reward를 역순으로하여 미래가치로 할인을 수행\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
    "            returns[t] = running_return\n",
    "            \n",
    "#         print(returns)\n",
    "#         tensor([ 1.6557e+01,  1.5714e+01,  1.4863e+01,  1.4003e+01,  1.3134e+01,\n",
    "#          1.2257e+01,  1.1370e+01,  1.0475e+01,  9.5708e+00,  8.6574e+00,\n",
    "#          7.7348e+00,  6.8028e+00,  5.8614e+00,  4.9105e+00,  3.9500e+00,\n",
    "#          2.9798e+00,  1.9998e+00,  1.0099e+00,  1.0000e-02, -1.0000e+00])\n",
    "\n",
    "        # 정책 그라디언트를 학습하기 위한 신경망의 input으로 매 agent의 step별 state를 투입\n",
    "        policies = net(states)\n",
    "        policies = policies.view(-1, net.num_outputs)\n",
    "        \n",
    "#      https://subinium.github.io/pytorch-Tensor-Variable/\n",
    "#      detach reason: pytorch에서 텐서를 복제하기 위한 방법. 복사하면서 이전 텐서의 gradient에는 영향을 미치지 않기 위해서 사용함.\n",
    "        log_policies = (torch.log(policies) * actions.detach()).sum(dim=1)\n",
    "        \n",
    "#     정책들 중에서도 선택된 action에 대한 정책만을 선정하여 loss 값에 사용\n",
    "        loss = (-log_policies * returns).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # 행동을 return하는 함수\n",
    "    def get_action(self, input):\n",
    "        policy = self.forward(input)\n",
    "        # 왼쪽(0)과 오른쪽(1) 가속을 할 확률을 return 받음\n",
    "        # 0: 왼쪽인 확률을 기준으로 랜덤으로 다음 action을 선정 => 이는 exploration 과 exploitng 사이의 균형을 맞추기 위함\n",
    "        policy = policy[0].data.numpy()\n",
    "\n",
    "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4\n",
      "action size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\kwon2\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.seed(500)\n",
    "torch.manual_seed(500)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('state size:', num_inputs)\n",
    "print('action size:', num_actions)\n",
    "\n",
    "net = QNet(num_inputs, num_actions)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "net.to(device)\n",
    "net.train()\n",
    "running_score = 0\n",
    "# steps = 0\n",
    "loss = 0\n",
    "\n",
    "# 1: \n",
    "for e in range(3000):\n",
    "    done = False\n",
    "    memory = Memory()\n",
    "\n",
    "    steps = 0\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    state = state.unsqueeze(0)\n",
    "\n",
    "    # 2: 신경망에서 각 상태에 대해 확률의 평균값을 얻기\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        #현재 State를 가지고 다음에 취하게 될 action을 확률로 추정한 후 그 확률을 가지고 random하게 다음 action을 선정\n",
    "        # next_state(obs)는 아래와 같이 구성\n",
    "        # * 수평위치\n",
    "        # * 카트의 속도\n",
    "        # * 막대의 각도\n",
    "        # * 막대의 각속도\n",
    "        action = net.get_action(state)\n",
    "        \n",
    "        # 추정한 action을 가지고 수행 \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = torch.Tensor(next_state)\n",
    "        # 1차원 -> 2차원으로 확장 (행 개념 ) (4,) => (1,4)\n",
    "        next_state = next_state.unsqueeze(0)\n",
    "\n",
    "        mask = 0 if done else 1\n",
    "        reward = reward if not done or score == 499 else -1\n",
    "\n",
    "        # 0과 1중 action이 취해진 항목에 대해 1로 값을 반영\n",
    "        action_one_hot = torch.zeros(2)\n",
    "        action_one_hot[action] = 1\n",
    "        # 메모리에  현재 state, 다음 state, 현재 state에서 취한 행동, 보상, mask를 기록\n",
    "        memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "        # 학습 상황을 보기 위해  점수(== 보상) 출력\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    break\n",
    "    # 폴이 쓰러져서 한 eposiode가 끝난 경우, 그에 대한 학습을 수행\n",
    "    loss = QNet.train_model(net, memory.sample(), optimizer)\n",
    "\n",
    "    score = score if score == 500.0 else score + 1\n",
    "    running_score = 0.99 * running_score + 0.01 * score\n",
    "    if e % log_interval == 0:\n",
    "        print('{} episode | steps: {} |score: {:.2f}'.format(\n",
    "            e, steps ,running_score))\n",
    "#         writer.add_scalar('log/score', float(running_score), e)\n",
    "#         writer.add_scalar('log/loss', float(loss), e)\n",
    "\n",
    "    if running_score > goal_score:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
